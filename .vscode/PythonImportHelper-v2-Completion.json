[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "spacy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "spacy",
        "description": "spacy",
        "detail": "spacy",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "get_density",
        "importPath": "density_consistency",
        "description": "density_consistency",
        "isExtraImport": true,
        "detail": "density_consistency",
        "documentation": {}
    },
    {
        "label": "clean_linebreaks",
        "kind": 2,
        "importPath": "src.cleanup_preprocess_USCSoriginal",
        "description": "src.cleanup_preprocess_USCSoriginal",
        "peekOfCode": "def clean_linebreaks(s):\n    if s==None: return None\n    s=str(s)\n    s = s.replace('\\x08', '')\n    s = s.strip('\\r\\n')\n    s = s.replace('\\r\\n', '-')\n    s = s.replace(' ', '')\n    return s\nf2['USCS_clean']=f2['USCS'].apply(clean_linebreaks)\nuscs_format=[\"GW\", \"GP\", \"GM\", \"GC\", \"SW\", \"SP\", \"SM\", \"SC\", \"ML\", \"CL\", \"OL\", \"MH\", \"CH\", \"OH\", \"PT\"]",
        "detail": "src.cleanup_preprocess_USCSoriginal",
        "documentation": {}
    },
    {
        "label": "clean_validate",
        "kind": 2,
        "importPath": "src.cleanup_preprocess_USCSoriginal",
        "description": "src.cleanup_preprocess_USCSoriginal",
        "peekOfCode": "def clean_validate(s, l_format=uscs_format, dict_group=uscs_group_map):\n    if s==None: return None\n    if s.count('-') > 1:\n        return None\n    if ',' in s:\n        return None\n    if 'No' in s or 'nan' in s:\n        return None\n    if '-' in s:\n        part1, part2 = s.split('-')",
        "detail": "src.cleanup_preprocess_USCSoriginal",
        "documentation": {}
    },
    {
        "label": "clean_joint",
        "kind": 2,
        "importPath": "src.cleanup_preprocess_USCSoriginal",
        "description": "src.cleanup_preprocess_USCSoriginal",
        "peekOfCode": "def clean_joint(s):\n    if s==None: return None\n    if '-' not in s:\n        return s\n    if s == 'SP-SM':\n        return s\n    return s.split('-')[0]\nf2['USCS_clean']=f2['USCS_clean'].apply(clean_joint)\nf2.drop(columns=['USCS'], inplace=True, axis=1)\nf2=f2.rename(columns={'USCS_clean': 'USCS'})",
        "detail": "src.cleanup_preprocess_USCSoriginal",
        "documentation": {}
    },
    {
        "label": "uscs_group_map",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_USCSoriginal",
        "description": "src.cleanup_preprocess_USCSoriginal",
        "peekOfCode": "uscs_group_map = {\n    \"GW\": \"gravel\",\n    \"GP\": \"gravel\",\n    \"GM\": \"gravel\",\n    \"GC\": \"gravel\",\n    \"SW\": \"sand\",\n    \"SP\": \"sand\",\n    \"SM\": \"sand\",\n    \"SC\": \"sand\",\n    \"ML\": \"silt\",",
        "detail": "src.cleanup_preprocess_USCSoriginal",
        "documentation": {}
    },
    {
        "label": "lemmatize_plurals_only",
        "kind": 2,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "def lemmatize_plurals_only(text):\n    words = word_tokenize(text)\n    lemmatized_words = [lemmatizer.lemmatize(word, pos='n') for word in words]\n    return \" \".join(lemmatized_words)\ndef match_comprehensive(description, lithology, standards, tbd_standards, other_standards, map_standards):\n    description = description.lower()\n    description = lemmatize_plurals_only(description)\n    lithology = lithology.lower()\n    if lithology in map_standards.keys():\n        return map_standards[lithology]",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "match_comprehensive",
        "kind": 2,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "def match_comprehensive(description, lithology, standards, tbd_standards, other_standards, map_standards):\n    description = description.lower()\n    description = lemmatize_plurals_only(description)\n    lithology = lithology.lower()\n    if lithology in map_standards.keys():\n        return map_standards[lithology]\n    elif lithology in other_standards:\n        return \"other\"\n    elif lithology in tbd_standards.keys():\n        uscs_backups=tbd_standards[lithology]",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "match_map",
        "kind": 2,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "def match_map(row):\n    return match_comprehensive(row[\"Description\"], row[\"Lithology\"], dict_uscsRules_keyword, tbd_categories, other_categories, uscs_mapping)\n# apply & cleanup\nf2['USCSre_desc']=f2.apply(match_map, axis=1)\nf2['USCSre_desc']=f2['Lithology'].map(dictlith_other).fillna(f2['USCSre_desc'])\nf2=f2[f2['USCSre_desc'] != 'incorrect']\nf2=f2[['Borehole ID', 'Sub Borehole Layer', 'Top Depth', 'Bottom Depth', 'USCSre_desc']]\nf2.columns=['Borehole ID', 'Sub Borehole Layer', 'Top Depth', 'Bottom Depth', 'USCS']\nf2.to_csv(path_export, index=False)",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "uscs_mapping_o",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "uscs_mapping_o = {\n    \"Sandy silt\": \"ML\", \"Silty sand\": \"SM\", \"Silty clay\": \"CL\", \"Clayey silt\": \"ML\",\n    \"Clayey sand\": \"SC\", \"Peat\": \"PT\", \"Sandy clay\": \"CL\", \"Silty gravel\": \"GM\",\n    \"Gravelly clay\": \"CL\", \"Clayey gravel\": \"GC\"\n}\nuscs_mapping={k.lower():v for k,v in uscs_mapping_o.items()}\nother_categories = {\n    \"Asphalt / concrete\", \"Topsoil / vegetation\", \"Fill\", \"Debris\",\n    \"Sedimentary bedrock\", \"Undifferentiated rock\", \"Cobbles / boulders\",\n    \"Plutonic bedrock\", \"Volcanic ash\", \"Volcanic bedrock\", \"Metamorphic bedrock\"",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "other_categories",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "other_categories = {\n    \"Asphalt / concrete\", \"Topsoil / vegetation\", \"Fill\", \"Debris\",\n    \"Sedimentary bedrock\", \"Undifferentiated rock\", \"Cobbles / boulders\",\n    \"Plutonic bedrock\", \"Volcanic ash\", \"Volcanic bedrock\", \"Metamorphic bedrock\"\n}\nother_categories={k.lower() for k in other_categories}\ntbd_categories = {\n    \"silt\": [\"ML\", \"OL\", \"MH\", \"OH\"],\n    \"clay\": [\"CL\", \"OL\", \"CH\", \"OH\"],\n    \"sand\": [\"SW\", \"SP\", \"SM\", \"SC\"],",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "tbd_categories",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "tbd_categories = {\n    \"silt\": [\"ML\", \"OL\", \"MH\", \"OH\"],\n    \"clay\": [\"CL\", \"OL\", \"CH\", \"OH\"],\n    \"sand\": [\"SW\", \"SP\", \"SM\", \"SC\"],\n    \"gravel\": [\"GW\", \"GP\", \"GM\", \"GC\"]\n}\ntbd_categories={k.lower():v for k,v in tbd_categories.items()}\ndict_uscsRules_keyword = {\n    \"GW\": {\"ukeywords\": [\"well graded gravel\"], \n           \"keywords\": [\"gravel sand mixture\", \"sand gravel cobble mixture\", \"little fine\", \"no fine\"], ",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "dict_uscsRules_keyword",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "dict_uscsRules_keyword = {\n    \"GW\": {\"ukeywords\": [\"well graded gravel\"], \n           \"keywords\": [\"gravel sand mixture\", \"sand gravel cobble mixture\", \"little fine\", \"no fine\"], \n           \"group\": \"gravel\"},\n    \"GP\": {\"ukeywords\": [\"poorly graded gravel\"], \n           \"keywords\": [\"gravel sand mixture\", \"sand gravel cobble mixture\", \"little fine\", \"no fine\"], \n           \"group\": \"gravel\"},\n    \"GM\": {\"ukeywords\": [\"silty gravel\", \"gravel sand silt mixture\"], \n           \"keywords\": [\"with fine\"], \n           \"group\": \"gravel\"},",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "dictlith_other",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "dictlith_other = {\n    \"Fill\": \"Fill\",\n    \"Asphalt / concrete\": \"Asphalt or concrete\",\n    \"Topsoil / vegetation\": \"Topsoil or vegetation\",\n    \"Sedimentary bedrock\": \"Bedrock\",\n    \"Volcanic bedrock\": \"Bedrock\",\n    \"Debris\": \"Fill\",\n    \"Cobbles / boulders\": \"GP\",\n    \"Undifferentiated rock\": \"Bedrock\",\n    \"Volcanic ash\": \"Volcanic ash\",",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "lemmatizer",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "lemmatizer = WordNetLemmatizer()\nnlp = spacy.load(\"en_core_web_md\")\ndef lemmatize_plurals_only(text):\n    words = word_tokenize(text)\n    lemmatized_words = [lemmatizer.lemmatize(word, pos='n') for word in words]\n    return \" \".join(lemmatized_words)\ndef match_comprehensive(description, lithology, standards, tbd_standards, other_standards, map_standards):\n    description = description.lower()\n    description = lemmatize_plurals_only(description)\n    lithology = lithology.lower()",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_USCSreprediction",
        "description": "src.cleanup_preprocess_USCSreprediction",
        "peekOfCode": "nlp = spacy.load(\"en_core_web_md\")\ndef lemmatize_plurals_only(text):\n    words = word_tokenize(text)\n    lemmatized_words = [lemmatizer.lemmatize(word, pos='n') for word in words]\n    return \" \".join(lemmatized_words)\ndef match_comprehensive(description, lithology, standards, tbd_standards, other_standards, map_standards):\n    description = description.lower()\n    description = lemmatize_plurals_only(description)\n    lithology = lithology.lower()\n    if lithology in map_standards.keys():",
        "detail": "src.cleanup_preprocess_USCSreprediction",
        "documentation": {}
    },
    {
        "label": "generate_sampling_depths",
        "kind": 2,
        "importPath": "src.cleanup_preprocess_layersampling",
        "description": "src.cleanup_preprocess_layersampling",
        "peekOfCode": "def generate_sampling_depths(top, bottom, total, grid=4):\n    top=min(top, bottom)\n    bottom=max(top, bottom)\n    if total<=2*grid:\n        return [(top+bottom)/2]\n    elif total <= 4*grid:\n        return [(2*top+bottom)/3, (top+2*bottom)/3]\n    else:\n        return [top+grid, (top+bottom)/2, bottom-grid]\nexpanded_data = []",
        "detail": "src.cleanup_preprocess_layersampling",
        "documentation": {}
    },
    {
        "label": "expanded_data",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_layersampling",
        "description": "src.cleanup_preprocess_layersampling",
        "peekOfCode": "expanded_data = []\nfor _, row in f2.iterrows():\n    borehole_id = row['Borehole ID']\n    top_depth = row['Top Depth']\n    bottom_depth = row['Bottom Depth']\n    uscs = row['USCS']\n    total_depth = row['Total Depth']\n    sampling_depths = generate_sampling_depths(top_depth, bottom_depth, total_depth)\n    for depth in sampling_depths:\n        expanded_data.append({'Borehole ID': borehole_id, 'Sampling Depth': depth, 'USCS': uscs})",
        "detail": "src.cleanup_preprocess_layersampling",
        "documentation": {}
    },
    {
        "label": "df_expanded",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_layersampling",
        "description": "src.cleanup_preprocess_layersampling",
        "peekOfCode": "df_expanded = pd.DataFrame(expanded_data)\ndf_expanded = df_expanded.merge(f1, on='Borehole ID', how='left')\ndf_expanded['Sampling Elevation'] = df_expanded['Elevation'] - df_expanded['Sampling Depth']\ndf_sampling=df_expanded[['Borehole ID', 'Document ID', 'Latitude', 'Longitude', 'Elevation', 'Sampling Depth', 'Sampling Elevation', 'USCS']]\ndf_sampling.dropna()\ndf_sampling.to_csv(path_export, index=False)",
        "detail": "src.cleanup_preprocess_layersampling",
        "documentation": {}
    },
    {
        "label": "df_expanded",
        "kind": 5,
        "importPath": "src.cleanup_preprocess_layersampling",
        "description": "src.cleanup_preprocess_layersampling",
        "peekOfCode": "df_expanded = df_expanded.merge(f1, on='Borehole ID', how='left')\ndf_expanded['Sampling Elevation'] = df_expanded['Elevation'] - df_expanded['Sampling Depth']\ndf_sampling=df_expanded[['Borehole ID', 'Document ID', 'Latitude', 'Longitude', 'Elevation', 'Sampling Depth', 'Sampling Elevation', 'USCS']]\ndf_sampling.dropna()\ndf_sampling.to_csv(path_export, index=False)",
        "detail": "src.cleanup_preprocess_layersampling",
        "documentation": {}
    },
    {
        "label": "get_density",
        "kind": 2,
        "importPath": "src.density_consistency",
        "description": "src.density_consistency",
        "peekOfCode": "def get_density(uscs, sptn):\n    if type(uscs) != str: \n        return None\n    types = re.findall(r'\\b[A-Z]{2}\\b', uscs)\n    coarse, fine= False, False\n    for t in types:\n        if t in [\"GW\", \"GP\", \"GM\", \"GC\", \"SW\", \"SP\", \"SM\", \"SC\"]:\n            coarse = True\n            break\n        elif t in [\"ML\", \"CL\", \"OL\", \"MH\", \"CH\", \"OH\", \"PT\"]:",
        "detail": "src.density_consistency",
        "documentation": {}
    },
    {
        "label": "get_filtered_boreholes",
        "kind": 2,
        "importPath": "src.sptn",
        "description": "src.sptn",
        "peekOfCode": "def get_filtered_boreholes(sptn, layering, max_depth_diff=10, sptn_threshold=45):\n    uscs_range = layering.groupby(\"Borehole ID\").agg(\n        borehole_top_depth=(\"Top Depth\", \"min\"),\n        borehole_bottom_depth=(\"Bottom Depth\", \"max\")\n    )\n    sptn_range = sptn.groupby(\"Borehole ID\")['Test Depth'].agg(\n        spt_top_depth=\"min\",\n        spt_bottom_depth=\"max\"\n    )\n    range = pd.merge(uscs_range, sptn_range, on=[\"Borehole ID\"], how=\"inner\")",
        "detail": "src.sptn",
        "documentation": {}
    },
    {
        "label": "merge_all",
        "kind": 2,
        "importPath": "src.sptn",
        "description": "src.sptn",
        "peekOfCode": "def merge_all(boreholes, layering, sptn, filtered_boreholes):\n    uscs_sptn = pd.merge(sptn, layering, on=[\"Borehole ID\"], how=\"inner\")\n    # If the test depth is right at the junction of two layers, it is assigned to the lower layer.\n    uscs_sptn = uscs_sptn[(uscs_sptn[\"Test Depth\"] >= uscs_sptn[\"Top Depth\"]) & \n                          (uscs_sptn[\"Test Depth\"] < uscs_sptn[\"Bottom Depth\"])]\n    # If the test depth is right at the junction of two layers, it is assigned to both layers.\n    # uscs_sptn = uscs_sptn[(uscs_sptn[\"Test Depth\"] >= uscs_sptn[\"Top Depth\"]) &\n    #                       (uscs_sptn[\"Test Depth\"] <= uscs_sptn[\"Bottom Depth\"])]\n    uscs_sptn_filtered = uscs_sptn[uscs_sptn[\"Borehole ID\"].isin(filtered_boreholes)]\n    boreholes_uscs_sptn = pd.merge(boreholes, uscs_sptn_filtered, on=[\"Borehole ID\"], how=\"right\")",
        "detail": "src.sptn",
        "documentation": {}
    }
]